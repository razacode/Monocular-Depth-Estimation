# Monocular Depth Estimation

## What is Knowledge Distillation 

Itâ€™s the procedure or the process for reducing the model complexity and computation overhead while maintaining the performance same as originally. One of the feasible way is to quantize the model, prune the redundant parameters and many more.

