# Monocular Depth Estimation

## What is Knowledge Distillation 

Itâ€™s the procedure or the process for reducing the model complexity and computation overhead while maintaining the performance same as originally. One of the feasible way is to quantize the model, prune the redundant parameters and many more.

## Dataset
Here some sample images from the Market 1501 dataset. 
![dataset]([https://github.com/razacode/Attribute-recognition-and-reidentification-Market1501-dataset/blob/main/img/dataset-1.png](https://github.com/razacode/Monocular-Depth-Estimation/blob/main/References%20in%20bibliography/mde1.PNG)
Each subject is captured from multiple cameras in different moments. Are present also some junk images or distractors as you can observe in the last row

## Dependencies

Python

```
$ sudo apt-get install python3 python3-pip
```

PyTorch

```
$ pip install pytorch
# $ conda install pytorch
```

Tensorflow

```
$ pip install tensorflow
# $ conda install -c conda-forge tensorflow
```

NumPy

```
$ pip install numpy
# $ conda install numpy
```

Pandas

```
$ pip install pandas
# $ conda install pandas
```

Matplotlib

```
$ pip install matplotlib
# $ conda install matplotlib
```

